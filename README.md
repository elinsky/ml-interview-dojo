# ML Interview Dojo

Personal repository for ML interview prep flashcards.

## Tier System

| Tier | Name | Criteria |
|------|------|----------|
| 0 | â˜‘ï¸ Attempted | Tried but couldn't recall |
| 1 | ğŸ‘ Recalled | Partial recall, or used hints/peeked |
| 2 | ğŸ’ª Independent | Full recall, no hints, no peeking |
| 3 | ğŸ† Mastered | Independent + answered in â‰¤2 min |
| - | â­ New | Not yet attempted |

## Progress Summary

**Mastery Progress:** [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 17.6% (19/108)

| Status | Count |
|--------|-------|
| ğŸ† Mastered | 19 |
| ğŸ’ª Independent | 0 |
| ğŸ‘ Recalled | 24 |
| â˜‘ï¸ Attempted | 5 |
| â­ New | 60 |
| **Total** | **108** |

## Quick Start

```bash
# Log an attempt
python3 scripts/log_attempt.py --file "ml-rapid-fire/classical-ml/logistic-regression/01-what-is-logistic-regression.md" --time 2 --hints false --looked false --recall full

# Update this README
python3 scripts/generate_readme.py
```

## Flashcards

### Classical Ml

**Progress:** [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 44.4% (4/9)

- ğŸ‘ [What is Logistic Regression](ml-rapid-fire/classical-ml/logistic-regression/01-what-is-logistic-regression.md) `1 attempts`
- ğŸ† [Why Not Linear Regression](ml-rapid-fire/classical-ml/logistic-regression/02-why-not-linear-regression.md) `1 attempts`
- â˜‘ï¸ [Sigmoid Function](ml-rapid-fire/classical-ml/logistic-regression/03-sigmoid-function.md) `1 attempts`
- ğŸ† [How Trained](ml-rapid-fire/classical-ml/logistic-regression/04-how-trained.md) `1 attempts`
- ğŸ‘ [Coefficient Interpretation](ml-rapid-fire/classical-ml/logistic-regression/05-coefficient-interpretation.md) `1 attempts`
- ğŸ‘ [Decision Boundary](ml-rapid-fire/classical-ml/logistic-regression/06-decision-boundary.md) `1 attempts`
- ğŸ† [Multi-class](ml-rapid-fire/classical-ml/logistic-regression/07-multi-class.md) `1 attempts`
- ğŸ† [Loss Function](ml-rapid-fire/classical-ml/logistic-regression/08-loss-function.md) `1 attempts`
- â­ [Linearly Separable](ml-rapid-fire/classical-ml/logistic-regression/09-linearly-separable.md)

### Clustering

**Progress:** [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0.0% (0/9)

- â­ [What is Clustering](ml-rapid-fire/classical-ml/clustering/01-what-is-clustering.md)
- â­ [K-Means Clustering](ml-rapid-fire/classical-ml/clustering/02-k-means.md)
- â­ [K-Means++ Initialization](ml-rapid-fire/classical-ml/clustering/03-k-means-plus-plus.md)
- â­ [Choosing K](ml-rapid-fire/classical-ml/clustering/04-choosing-k.md)
- â­ [DBSCAN](ml-rapid-fire/classical-ml/clustering/05-dbscan.md)
- â­ [Hierarchical Clustering](ml-rapid-fire/classical-ml/clustering/06-hierarchical-clustering.md)
- â­ [Linkage Methods](ml-rapid-fire/classical-ml/clustering/07-linkage-methods.md)
- â­ [Clustering Evaluation](ml-rapid-fire/classical-ml/clustering/08-clustering-evaluation.md)
- â­ [Clustering Algorithm Comparison](ml-rapid-fire/classical-ml/clustering/09-comparison.md)

### Cross Entropy

**Progress:** [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 25.0% (2/8)

- ğŸ‘ [01-what-is-cross-entropy](ml-rapid-fire/classical-ml/cross-entropy/01-what-is-cross-entropy.md) `1 attempts`
- â˜‘ï¸ [02-why-not-mse](ml-rapid-fire/classical-ml/cross-entropy/02-why-not-mse.md) `1 attempts`
- â˜‘ï¸ [03-binary-cross-entropy-formula](ml-rapid-fire/classical-ml/cross-entropy/03-binary-cross-entropy-formula.md) `1 attempts`
- ğŸ† [04-confident-correct](ml-rapid-fire/classical-ml/cross-entropy/04-confident-correct.md) `1 attempts`
- ğŸ‘ [05-confident-wrong](ml-rapid-fire/classical-ml/cross-entropy/05-confident-wrong.md) `1 attempts`
- â˜‘ï¸ [06-categorical-cross-entropy](ml-rapid-fire/classical-ml/cross-entropy/06-categorical-cross-entropy.md) `1 attempts`
- ğŸ‘ [07-calculate-cross-entropy](ml-rapid-fire/classical-ml/cross-entropy/07-calculate-cross-entropy.md) `1 attempts`
- ğŸ† [08-calculate-categorical-cross-entropy](ml-rapid-fire/classical-ml/cross-entropy/08-calculate-categorical-cross-entropy.md) `1 attempts`

### Decision Trees

**Progress:** [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0.0% (0/16)

- â­ [What is a Decision Tree](ml-rapid-fire/classical-ml/decision-trees/01-what-is-decision-tree.md)
- â­ [How Decision Trees Split](ml-rapid-fire/classical-ml/decision-trees/02-how-splits-decided.md)
- â­ [Gini Impurity](ml-rapid-fire/classical-ml/decision-trees/03-gini-impurity.md)
- â­ [Information Gain / Entropy](ml-rapid-fire/classical-ml/decision-trees/04-information-gain.md)
- â­ [Preventing Overfitting in Decision Trees](ml-rapid-fire/classical-ml/decision-trees/05-prevent-overfitting.md)
- â­ [Decision Tree Pros and Cons](ml-rapid-fire/classical-ml/decision-trees/06-pros-cons.md)
- â­ [Ensemble Methods](ml-rapid-fire/classical-ml/decision-trees/07-ensemble-methods.md)
- â­ [Bagging](ml-rapid-fire/classical-ml/decision-trees/08-bagging.md)
- â­ [Random Forest](ml-rapid-fire/classical-ml/decision-trees/09-random-forest.md)
- â­ [Why Random Forest Reduces Variance](ml-rapid-fire/classical-ml/decision-trees/10-why-rf-reduces-variance.md)
- â­ [Boosting](ml-rapid-fire/classical-ml/decision-trees/11-boosting.md)
- â­ [Bagging vs Boosting](ml-rapid-fire/classical-ml/decision-trees/12-bagging-vs-boosting.md)
- â­ [AdaBoost](ml-rapid-fire/classical-ml/decision-trees/13-adaboost.md)
- â­ [Gradient Boosting](ml-rapid-fire/classical-ml/decision-trees/14-gradient-boosting.md)
- â­ [XGBoost](ml-rapid-fire/classical-ml/decision-trees/15-xgboost.md)
- â­ [Boosting Bias or Variance](ml-rapid-fire/classical-ml/decision-trees/16-boosting-bias-variance.md)

### Knn

**Progress:** [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0.0% (0/9)

- â­ [What is KNN](ml-rapid-fire/classical-ml/knn/01-what-is-knn.md)
- â­ [KNN Prediction](ml-rapid-fire/classical-ml/knn/02-prediction.md)
- â­ [Distance Metrics](ml-rapid-fire/classical-ml/knn/03-distance-metrics.md)
- â­ [Choosing K](ml-rapid-fire/classical-ml/knn/04-choosing-k.md)
- â­ [Curse of Dimensionality](ml-rapid-fire/classical-ml/knn/05-curse-of-dimensionality.md)
- â­ [Feature Scaling for KNN](ml-rapid-fire/classical-ml/knn/06-feature-scaling.md)
- â­ [Weighted KNN](ml-rapid-fire/classical-ml/knn/07-weighted-knn.md)
- â­ [KNN Computational Complexity](ml-rapid-fire/classical-ml/knn/08-computational-complexity.md)
- â­ [KNN Pros and Cons](ml-rapid-fire/classical-ml/knn/09-pros-cons.md)

### Linear Regression

**Progress:** [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 45.5% (5/11)

- ğŸ† [01-what-is-linear-regression](ml-rapid-fire/classical-ml/linear-regression/01-what-is-linear-regression.md) `1 attempts`
- ğŸ† [02-simple-vs-multiple](ml-rapid-fire/classical-ml/linear-regression/02-simple-vs-multiple.md) `1 attempts`
- ğŸ‘ [03-linear-regression-formula](ml-rapid-fire/classical-ml/linear-regression/03-linear-regression-formula.md) `1 attempts`
- ğŸ† [04-loss-function](ml-rapid-fire/classical-ml/linear-regression/04-loss-function.md) `1 attempts`
- ğŸ‘ [05-ordinary-least-squares](ml-rapid-fire/classical-ml/linear-regression/05-ordinary-least-squares.md) `1 attempts`
- â˜‘ï¸ [06-gradient-descent-for-lr](ml-rapid-fire/classical-ml/linear-regression/06-gradient-descent-for-lr.md) `1 attempts`
- ğŸ† [07-coefficient-interpretation](ml-rapid-fire/classical-ml/linear-regression/07-coefficient-interpretation.md) `1 attempts`
- ğŸ‘ [08-assumptions](ml-rapid-fire/classical-ml/linear-regression/08-assumptions.md) `1 attempts`
- ğŸ‘ [09-ridge-vs-lasso](ml-rapid-fire/classical-ml/linear-regression/09-ridge-vs-lasso.md) `1 attempts`
- ğŸ‘ [10-r-squared](ml-rapid-fire/classical-ml/linear-regression/10-r-squared.md) `1 attempts`
- ğŸ† [11-mse-formula](ml-rapid-fire/classical-ml/linear-regression/11-mse-formula.md) `1 attempts`

### Ml Basics

**Progress:** [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 38.1% (8/21)

- ğŸ† [01-what-is-ml](ml-rapid-fire/fundamentals/ml-basics/01-what-is-ml.md) `1 attempts`
- ğŸ† [02-traditional-vs-ml](ml-rapid-fire/fundamentals/ml-basics/02-traditional-vs-ml.md) `1 attempts`
- ğŸ‘ [03-three-components](ml-rapid-fire/fundamentals/ml-basics/03-three-components.md) `1 attempts`
- ğŸ† [04-types-of-learning](ml-rapid-fire/fundamentals/ml-basics/04-types-of-learning.md) `1 attempts`
- ğŸ‘ [05-classification-vs-regression](ml-rapid-fire/fundamentals/ml-basics/05-classification-vs-regression.md) `1 attempts`
- ğŸ† [06-what-is-a-model](ml-rapid-fire/fundamentals/ml-basics/06-what-is-a-model.md) `1 attempts`
- ğŸ† [07-what-is-loss-function](ml-rapid-fire/fundamentals/ml-basics/07-what-is-loss-function.md) `1 attempts`
- ğŸ‘ [08-what-is-optimization](ml-rapid-fire/fundamentals/ml-basics/08-what-is-optimization.md) `1 attempts`
- ğŸ‘ [09-loss-vs-metric](ml-rapid-fire/fundamentals/ml-basics/09-loss-vs-metric.md) `1 attempts`
- ğŸ‘ [10-params-vs-hyperparams](ml-rapid-fire/fundamentals/ml-basics/10-params-vs-hyperparams.md) `1 attempts`
- ğŸ† [11-what-is-regularization](ml-rapid-fire/fundamentals/ml-basics/11-what-is-regularization.md) `1 attempts`
- ğŸ† [12-training-vs-inference](ml-rapid-fire/fundamentals/ml-basics/12-training-vs-inference.md) `1 attempts`
- ğŸ† [13-overfitting-underfitting](ml-rapid-fire/fundamentals/ml-basics/13-overfitting-underfitting.md) `1 attempts`
- ğŸ‘ [14-bias-variance](ml-rapid-fire/fundamentals/ml-basics/14-bias-variance.md) `1 attempts`
- ğŸ‘ [15-when-to-use-ml](ml-rapid-fire/fundamentals/ml-basics/15-when-to-use-ml.md) `1 attempts`
- ğŸ‘ [Bias-Variance Decomposition](ml-rapid-fire/fundamentals/ml-basics/16-bias-variance-decomposition.md) `1 attempts`
- ğŸ‘ [Diagnosing Bias vs Variance](ml-rapid-fire/fundamentals/ml-basics/17-diagnosing-bias-variance.md) `1 attempts`
- ğŸ‘ [Causes of High Variance](ml-rapid-fire/fundamentals/ml-basics/18-causes-high-variance.md) `1 attempts`
- ğŸ‘ [How to Reduce Variance](ml-rapid-fire/fundamentals/ml-basics/19-reduce-variance.md) `1 attempts`
- ğŸ‘ [Causes of High Bias](ml-rapid-fire/fundamentals/ml-basics/20-causes-high-bias.md) `1 attempts`
- ğŸ‘ [How to Reduce Bias](ml-rapid-fire/fundamentals/ml-basics/21-reduce-bias.md) `1 attempts`

### Naive Bayes

**Progress:** [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0.0% (0/9)

- â­ [What is Naive Bayes](ml-rapid-fire/classical-ml/naive-bayes/01-what-is-naive-bayes.md)
- â­ [Bayes Theorem](ml-rapid-fire/classical-ml/naive-bayes/02-bayes-theorem.md)
- â­ [The Naive Assumption](ml-rapid-fire/classical-ml/naive-bayes/03-naive-assumption.md)
- â­ [Types of Naive Bayes](ml-rapid-fire/classical-ml/naive-bayes/04-types-of-naive-bayes.md)
- â­ [Gaussian Naive Bayes](ml-rapid-fire/classical-ml/naive-bayes/05-gaussian-naive-bayes.md)
- â­ [Multinomial Naive Bayes](ml-rapid-fire/classical-ml/naive-bayes/06-multinomial-naive-bayes.md)
- â­ [Laplace Smoothing](ml-rapid-fire/classical-ml/naive-bayes/07-laplace-smoothing.md)
- â­ [Log Probabilities](ml-rapid-fire/classical-ml/naive-bayes/08-log-probabilities.md)
- â­ [Naive Bayes Pros and Cons](ml-rapid-fire/classical-ml/naive-bayes/09-pros-cons.md)

### Regularization

**Progress:** [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0.0% (0/7)

- â­ [L2 (Ridge) Penalty Formula](ml-rapid-fire/classical-ml/regularization/01-l2-ridge-formula.md)
- â­ [L1 (Lasso) Penalty Formula](ml-rapid-fire/classical-ml/regularization/02-l1-lasso-formula.md)
- â­ [Why L1 Produces Sparsity](ml-rapid-fire/classical-ml/regularization/03-why-l1-sparse.md)
- â­ [Why L2 Only Shrinks Weights](ml-rapid-fire/classical-ml/regularization/04-why-l2-shrinks.md)
- â­ [When to Choose L1 vs L2](ml-rapid-fire/classical-ml/regularization/05-when-l1-vs-l2.md)
- â­ [Elastic Net](ml-rapid-fire/classical-ml/regularization/06-elastic-net.md)
- â­ [Regularization Hyperparameter](ml-rapid-fire/classical-ml/regularization/07-lambda-hyperparameter.md)

### Svm

**Progress:** [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0.0% (0/9)

- â­ [What is an SVM](ml-rapid-fire/classical-ml/svm/01-what-is-svm.md)
- â­ [Margin and Support Vectors](ml-rapid-fire/classical-ml/svm/02-margin-support-vectors.md)
- â­ [Hard Margin vs Soft Margin](ml-rapid-fire/classical-ml/svm/03-hard-vs-soft-margin.md)
- â­ [The Kernel Trick](ml-rapid-fire/classical-ml/svm/04-kernel-trick.md)
- â­ [Common Kernel Functions](ml-rapid-fire/classical-ml/svm/05-common-kernels.md)
- â­ [The C Parameter](ml-rapid-fire/classical-ml/svm/06-c-parameter.md)
- â­ [The Gamma Parameter](ml-rapid-fire/classical-ml/svm/07-gamma-parameter.md)
- â­ [SVM Pros and Cons](ml-rapid-fire/classical-ml/svm/08-pros-cons.md)
- â­ [Multiclass SVM](ml-rapid-fire/classical-ml/svm/09-multiclass.md)

---

*Last updated: 2025-11-28 18:51:37*