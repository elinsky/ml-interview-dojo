# ML Interview Dojo

Personal repository for preparing for machine learning interviews.

## Source Material

Questions are from [Machine Learning Interviews Book](https://huyenchip.com/ml-interviews-book/) by Chip Huyen:
- Questions: https://github.com/chiphuyen/ml-interviews-book/tree/master/contents

## Progress Scorecard

### Overall Mastery

```
[‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 0.0% (0/23)
```

### Card Status

| Status | Count | Percentage |
|--------|-------|------------|
| üèÜ Mastered | 0 | 0.0% |
| üí™ Learning | 0 | 0.0% |
| ‚≠ê New | 23 | 100.0% |
| **Total** | **23** | **100%** |


## Quick Start

```bash
# Start a review session
python3 srs.py review

# View your stats
python3 srs.py stats

# Update this README
python3 scripts/generate_readme.py
```

## How the SRS Works

This uses a **queue-based spaced repetition system** (not date-based like Anki):

- **No due dates**: Cards are prioritized in a smart queue based on performance
- **No pile-up**: Come back after months and continue right where you left off
- **Adaptive**: Struggles with a card? It gets higher priority automatically
- **Mastery tracking**: Cards graduate from New ‚Üí Learning ‚Üí Mastered

**Mastery Criteria**: 3+ consecutive reviews with score ‚â•4, spaced at least 1 hour apart

## Status Legend

- üèÜ **Mastered**: Consistently perfect recall
- üí™ **Strong**: Average score ‚â•4.0, still in practice
- üëç **Learning**: Average score ‚â•3.0, making progress
- üìñ **Practicing**: Reviewed but needs more work
- ‚≠ê **New**: Not yet reviewed

## Topics by Category


### 01 Basics

**Progress:** [‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 0.0% (0/23) | üèÜ 0 | üí™ 0 | ‚≠ê 23

- ‚≠ê [01-learning-types](ml-workflows/01-basics/01-learning-types.md): Explain supervised, unsupervised, weakly supervised, semi-supervised, and active...
- ‚≠ê [02-empirical-risk](ml-workflows/01-basics/02-empirical-risk.md): What's the risk in empirical risk minimization?
- ‚≠ê [03-why-empirical](ml-workflows/01-basics/03-why-empirical.md): Why is it empirical?
- ‚≠ê [04-minimize-risk](ml-workflows/01-basics/04-minimize-risk.md): How do we minimize that risk?
- ‚≠ê [05-occams-razor](ml-workflows/01-basics/05-occams-razor.md): "Occam's razor states that when the simple explanation and complex explanation b...
- ‚≠ê [06-deep-learning-conditions](ml-workflows/01-basics/06-deep-learning-conditions.md): What are the conditions that allowed deep learning to gain popularity in the las...
- ‚≠ê [07-wide-vs-deep](ml-workflows/01-basics/07-wide-vs-deep.md): If we have a wide NN and a deep NN with the same number of parameters, which one...
- ‚≠ê [08-universal-approximation](ml-workflows/01-basics/08-universal-approximation.md): "The Universal Approximation Theorem states that a neural network with 1 hidden ...
- ‚≠ê [09-saddle-points-local-minima](ml-workflows/01-basics/09-saddle-points-local-minima.md): What are saddle points and local minima? Which are thought to cause more problem...
- ‚≠ê [10-parameters-vs-hyperparameters](ml-workflows/01-basics/10-parameters-vs-hyperparameters.md): What are the differences between parameters and hyperparameters?
- ‚≠ê [11-hyperparameter-tuning-importance](ml-workflows/01-basics/11-hyperparameter-tuning-importance.md): Why is hyperparameter tuning important?
- ‚≠ê [12-hyperparameter-tuning-algorithm](ml-workflows/01-basics/12-hyperparameter-tuning-algorithm.md): Explain the algorithm for tuning hyperparameters.
- ‚≠ê [13-classification-vs-regression](ml-workflows/01-basics/13-classification-vs-regression.md): What makes a classification problem different from a regression problem?
- ‚≠ê [14-classification-regression-conversion](ml-workflows/01-basics/14-classification-regression-conversion.md): Can a classification problem be turned into a regression problem and vice versa?
- ‚≠ê [15-parametric-vs-nonparametric](ml-workflows/01-basics/15-parametric-vs-nonparametric.md): What's the difference between parametric and non-parametric methods? Give exampl...
- ‚≠ê [16-when-parametric-vs-nonparametric](ml-workflows/01-basics/16-when-parametric-vs-nonparametric.md): When should we use parametric versus non-parametric methods?
- ‚≠ê [17-ensembling-benefits](ml-workflows/01-basics/17-ensembling-benefits.md): Why does ensembling independently trained models generally improve performance?
- ‚≠ê [18-l1-vs-l2-regularization](ml-workflows/01-basics/18-l1-vs-l2-regularization.md): Why does L1 regularization tend to lead to sparsity while L2 regularization push...
- ‚≠ê [19-production-degradation](ml-workflows/01-basics/19-production-degradation.md): Why does an ML model's performance degrade in production?
- ‚≠ê [20-deploying-large-models](ml-workflows/01-basics/20-deploying-large-models.md): What problems might we run into when deploying large machine learning models?
- ‚≠ê [21-production-performance-hypotheses](ml-workflows/01-basics/21-production-performance-hypotheses.md): What are your hypotheses about the causes of good test performance but poor prod...
- ‚≠ê [22-validate-hypotheses](ml-workflows/01-basics/22-validate-hypotheses.md): How do you validate whether your hypotheses are correct?
- ‚≠ê [23-address-causes](ml-workflows/01-basics/23-address-causes.md): How would you address the identified causes?

---

*Last updated: 2025-11-24 01:17:47*

*Generated by [scripts/generate_readme.py](scripts/generate_readme.py)*