# Contextual Bandits

**Q:** What are contextual bandits and how do they differ from standard bandits?
- Setup
- Relationship to RL
- Applications

**A:**

- **Setup**:
  - Before choosing arm, observe context (features) x
  - Reward depends on both arm and context
  - Learn policy: π(x) → arm
- **Relationship to RL**:
  - Standard bandits: no state, same distribution each round
  - Contextual: state (context) changes, but no state transitions from actions
  - Full RL: actions affect future states
- **Learning approaches**:
  - LinUCB: linear model of reward, UCB exploration
  - Thompson sampling with contextual posterior
  - Neural bandit: deep learning for context embedding
- **Applications**:
  - Ad personalization (context = user features, arm = which ad)
  - Recommendations (context = user/item, arm = which to show)
  - Clinical trials (context = patient, arm = treatment)
- **Finance**: portfolio selection conditioned on market regime

---
