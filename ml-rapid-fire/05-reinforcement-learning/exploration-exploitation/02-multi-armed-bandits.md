# Multi-Armed Bandits

**Q:** What is the multi-armed bandit problem?
- Setup
- Relationship to RL
- Why important

**A:**

- **Setup**:
  - k slot machines ("arms"), each with unknown reward distribution
  - At each step, choose one arm, receive reward
  - Goal: maximize total reward over T rounds
  - No state, just actions → simplified RL
- **Relationship to RL**:
  - Bandits: no state transitions, action affects only immediate reward
  - RL: actions affect future states and rewards
  - Bandits = RL with single state
- **Why important**:
  - Clean setting to study exploration-exploitation
  - Many real applications: A/B testing, recommendations, clinical trials
  - Foundations extend to full RL
- **Regret**: measure of suboptimality
  - R_T = Σ(μ* - μ_{a_t}) — difference from always playing best arm
  - Goal: minimize regret (sublinear growth)

---
