# Log Loss vs Cross-Entropy

**Q:** What is the relationship between log loss and cross-entropy?

**A:**

- Log loss = binary cross-entropy (same thing)
- Both measure divergence between distributions
- Higher penalty for confident wrong predictions
- Commonly used interchangeably

**See also:** [[202101171406 Log-Likelihood Cost]], [[202203050829 Kullback-Leibler Divergence]]

---
