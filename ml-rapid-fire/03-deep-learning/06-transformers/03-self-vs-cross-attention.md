# Self-Attention vs Cross-Attention

**Q:** What is self-attention and how does it differ from cross-attention?

**A:**

- **Self-attention**: Q, K, V all from same sequence
- **Cross-attention**: Q from one sequence, K/V from another
- Self-attention captures relationships within a sequence
- Cross-attention used in decoder to attend to encoder output
- Both use same attention formula

**See also:** [[202105270800 Transformers]], [[202105270808 Attention]]

---
