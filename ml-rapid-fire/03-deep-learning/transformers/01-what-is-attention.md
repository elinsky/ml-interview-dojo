# What is Attention?

**Q:** What is the attention mechanism and why was it introduced?

**A:**

- Allows model to focus on relevant parts of input sequence
- Solves seq2seq bottleneck (fixed-size context vector)
- Computes weighted sum of values based on query-key similarity
- Weights learned end-to-end via softmax
- Enables direct connections between distant positions

**See also:** [[202105270808 Attention]], [[202105270854 Bahdanau Attention]], [[202105270853 Luong Attention]]

---
