# GRU

**Q:** How does a GRU differ from LSTM?

**A:**

- **Gated Recurrent Unit**: simplified LSTM
- Only two gates: **reset** and **update** (vs three in LSTM)
- No separate cell state (merged with hidden state)
- Fewer parameters = faster training
- Similar performance to LSTM on many tasks

---
