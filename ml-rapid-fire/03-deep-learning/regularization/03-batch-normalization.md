# Batch Normalization

**Q:** What is batch normalization?

**A:**

- Normalizes layer inputs to zero mean, unit variance
- Adds learnable scale (gamma) and shift (beta) parameters
- Computed per mini-batch during training
- Uses running averages during inference
- Can eliminate bias term (beta replaces it)

---
