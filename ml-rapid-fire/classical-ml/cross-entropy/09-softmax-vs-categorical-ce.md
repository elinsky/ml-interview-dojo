# Softmax vs Categorical Cross-Entropy

**Q:** What is the difference between softmax and categorical cross-entropy?

**A:**
- **Softmax** is an **activation function** - converts logits into a probability distribution (sums to 1)
- **Categorical cross-entropy** is a **loss function** - measures how different predicted distribution is from true distribution

They're used together: softmax produces probabilities, categorical cross-entropy measures the error.

---
