# Boosting

**Q:** What is boosting?

**A:**

- Train **weak learners** (slightly better than random: 50%+ε) sequentially
- Each model corrects errors of previous
- Combine via **weighted sum** → produces **strong learner**
- Reduces **bias** primarily (also variance)
- Examples: AdaBoost, Gradient Boosting, XGBoost

[[202104161650]] [[202104171015]]

---
