# GPT vs BERT

**Q:** How does GPT differ from BERT?

**A:**

- **GPT**: decoder-only, autoregressive (left-to-right)
- **BERT**: encoder-only, bidirectional
- GPT uses causal masking (can't see future tokens)
- GPT pre-trains on next token prediction
- GPT uses GELU activation (not ReLU)

**See also:** [[202201240714 Summary - GPT-1]], [[202201231144 Summary - BERT]]

---
