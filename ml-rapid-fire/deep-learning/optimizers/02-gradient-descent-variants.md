# Gradient Descent Variants

**Q:** What are the three main gradient descent variants?

**A:**

- **Batch GD**: uses entire dataset per update (slow, stable)
- **Stochastic GD (SGD)**: uses one sample per update (noisy, fast)
- **Mini-batch GD**: uses small batch (best of both, most common)
- Batch size is a hyperparameter (typically 32, 64, 128)

---
