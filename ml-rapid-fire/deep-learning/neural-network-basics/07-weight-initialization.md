# Weight Initialization

**Q:** Why does weight initialization matter?

**A:**

- All zeros → all neurons learn same thing (symmetry problem)
- Too large → exploding gradients
- Too small → vanishing gradients
- **Xavier/Glorot**: good for sigmoid/tanh
- **He initialization**: good for ReLU

---
