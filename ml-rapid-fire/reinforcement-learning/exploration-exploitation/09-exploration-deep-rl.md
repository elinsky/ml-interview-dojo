# Exploration in Deep RL

**Q:** Why is exploration especially challenging in deep RL?
- Challenges
- Common approaches
- State of the art

**A:**

- **Challenges**:
  - Large state spaces: can't count visits
  - Function approximation: generalization affects exploration
  - Temporal abstraction: need to explore sequences, not just actions
- **Common approaches**:
  - ε-greedy: simple but inefficient in high dimensions
  - Parameter noise: add noise to network weights (NoisyNets)
  - Entropy bonus: reward policy entropy (SAC)
  - Intrinsic curiosity: reward for prediction error
- **State of the art**:
  - Random Network Distillation (RND): bonus for novel states
  - Go-Explore: archive promising states, return to explore from them
  - Model-based: use model uncertainty to guide exploration
- **Hard exploration benchmarks**: Montezuma's Revenge, Pitfall — require long-horizon exploration with sparse rewards

---
