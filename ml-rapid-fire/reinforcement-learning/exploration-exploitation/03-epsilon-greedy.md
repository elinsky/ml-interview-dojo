# Epsilon-Greedy

**Q:** How does epsilon-greedy exploration work?
- Algorithm
- Pros and cons
- Variants

**A:**

- **Algorithm**:
  - With probability 1-ε: take best known action (exploit)
  - With probability ε: take random action (explore)
  - ε typically 0.05-0.1
- **Pros**:
  - Simple to implement
  - Guaranteed exploration (always some chance of trying any action)
  - Easy to tune one parameter
- **Cons**:
  - Explores uniformly — wastes time on obviously bad actions
  - Doesn't use uncertainty information
  - Constant ε: explores forever even after convergence
- **Variants**:
  - **ε-decay**: start high (explore), decay over time (exploit more later)
  - Common schedule: ε_t = ε_0 / (1 + t/τ) or exponential decay
- **Used in**: Q-learning, DQN, many practical applications

---
